{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "import fiona\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from shapely.ops import unary_union\n",
    "from unidecode import unidecode\n",
    "import glob\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dg\n",
    "from dask.distributed import Client\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(log_path, log_entry):\n",
    "    # Check if the file exists to determine if headers need to be written\n",
    "    file_exists = os.path.isfile(log_path)\n",
    "\n",
    "    # Open the log file in append mode\n",
    "    with open(log_path, 'a', newline='') as csvfile:\n",
    "        log_writer = csv.writer(csvfile)\n",
    "\n",
    "        # Write the header if the file is new\n",
    "        if not file_exists:\n",
    "            log_writer.writerow(['Timestamp', 'HDENS_NAME', 'agglo_Id', 'uc_km2', 'gdf2_km2', 'gdf1_gdf2_km2', \n",
    "                                 'ua_km2', 'uagreen_km2', 'uagreen_urbc_km2', 'nqgreen_m2', 'green_not_covered_by_gdf1_m2',\n",
    "                                  'GQA_m2', 'GNA_m2', 'prDuration'])\n",
    "\n",
    "        # Write the log entry\n",
    "        log_writer.writerow(log_entry)\n",
    "\n",
    "def create_log_entry(val1, val2, val3, val4, val5, val6, val7, val8, val9, val10, val11, val12, processing_time):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%Hh%Mm%Ss')\n",
    "    processing_duration = str(processing_time)\n",
    "    return [timestamp, val1, val2, val3, val4, val5, val6, val7, val8, val9, val10, val11, val12, processing_duration]\n",
    "\n",
    "def parallel_overlay(gdf1, gdf2, npartitions=10, how='intersection'):\n",
    "    \"\"\"\n",
    "    Perform spatial overlay (intersection) using GeoPandas in parallel with Dask.\n",
    "    \n",
    "    Parameters:\n",
    "    gdf1 (GeoDataFrame): The first GeoDataFrame to overlay.\n",
    "    gdf2 (GeoDataFrame): The second GeoDataFrame to overlay.\n",
    "    npartitions (int): Number of partitions to use for Dask.\n",
    "    how (str): The type of overlay operation. Default is 'intersection'.\n",
    "    \n",
    "    Returns:\n",
    "    GeoDataFrame: The result of the overlay operation.\n",
    "    \"\"\"\n",
    "    # Convert GeoDataFrames to Dask GeoDataFrames\n",
    "    gdf1_dg = dg.from_geopandas(gdf1, npartitions=npartitions)\n",
    "    gdf2_dg = dg.from_geopandas(gdf2, npartitions=npartitions)\n",
    "\n",
    "    # Perform spatial overlay (intersection) using GeoPandas in parallel with Dask\n",
    "    gdf1_parts = [gdf1_dg.partitions[i].compute() for i in range(gdf1_dg.npartitions)]\n",
    "    gdf2_parts = [gdf2_dg.partitions[i].compute() for i in range(gdf2_dg.npartitions)]\n",
    "\n",
    "    # Manually overlay using GeoPandas\n",
    "    overlays = [gpd.overlay(gdf1_part, gdf2_part, how=how) for gdf1_part in gdf1_parts for gdf2_part in gdf2_parts]\n",
    "    result = gpd.GeoDataFrame(pd.concat(overlays, ignore_index=True))\n",
    "    gc.collect()\n",
    "    \n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ubach\\.conda\\envs\\geospatial_ETC_DI_v4\\lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 61281 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaunas\n",
      "2024-07-02 13:24:26.252825\n",
      "loading aglomeration city Kaunas\n",
      "agglomerationId_identifier\n",
      "ncm\n",
      "ncm_agl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q:\\UserTemp\\ubach\\AppData\\Local\\Temp\\63\\ipykernel_52220\\1119770620.py:45: UserWarning: `keep_geom_type=True` in overlay resulted in 3346 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  overlays = [gpd.overlay(gdf1_part, gdf2_part, how=how) for gdf1_part in gdf1_parts for gdf2_part in gdf2_parts]\n",
      "Q:\\UserTemp\\ubach\\AppData\\Local\\Temp\\63\\ipykernel_52220\\1119770620.py:45: UserWarning: `keep_geom_type=True` in overlay resulted in 1128 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  overlays = [gpd.overlay(gdf1_part, gdf2_part, how=how) for gdf1_part in gdf1_parts for gdf2_part in gdf2_parts]\n",
      "Q:\\UserTemp\\ubach\\AppData\\Local\\Temp\\63\\ipykernel_52220\\1119770620.py:45: UserWarning: `keep_geom_type=True` in overlay resulted in 480 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  overlays = [gpd.overlay(gdf1_part, gdf2_part, how=how) for gdf1_part in gdf1_parts for gdf2_part in gdf2_parts]\n",
      "c:\\Users\\ubach\\.conda\\envs\\geospatial_ETC_DI_v4\\lib\\site-packages\\geopandas\\geodataframe.py:1443: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncm_dis\n",
      "layers_ls\n",
      "ua\n",
      "green_not_covered_by_ncm\n",
      "GQA\n",
      "GQA_pts\n",
      "str(processing_time)\n"
     ]
    }
   ],
   "source": [
    "# Path to data folders\n",
    "indata_f = r'P:\\Environment and Health\\Noise\\ServiceContract\\2024_ServiceContract\\QuietAreas'\n",
    "outdata_f = os.path.join(indata_f, 'OutputData', 'test0207')\n",
    "if not os.path.exists(outdata_f):\n",
    "    # Create the folder if it doesn't exist\n",
    "    os.makedirs(outdata_f)\n",
    "\n",
    "# 0 PREPARE A LOG FILE FOR QC\n",
    "log_file = 'log_GQA_Step1.csv'\n",
    "log_path = os.path.join(outdata_f, log_file)\n",
    "\n",
    "# Initialize Dask client\n",
    "client = Client()\n",
    "\n",
    "# 1 READ URBAN CENTRES\n",
    "# Read shapefile\n",
    "uc_file_path = os.path.join(indata_f, 'UrbanCentres', 'HDC2021_RG.shp')\n",
    "# Read the GeoPackage file\n",
    "uc = gpd.read_file(uc_file_path)\n",
    "\n",
    "\n",
    "# 2 READ NOISE DATA\n",
    "# Load agglomerations delineations\n",
    "agls_file_path = os.path.join(indata_f, 'NoiseData', 'DF1_5_Agglomerations_20240429.gpkg')\n",
    "\n",
    "# Read the GeoPackage file\n",
    "agls = gpd.read_file(agls_file_path, layer = 'dbo.DF15_AgglomerationSource_Valid_LatestDelivery')\n",
    "\n",
    "cities_ls = ['Kaunas', 'København', 'Girona', 'Bordeaux']\n",
    "# Loop through test cities\n",
    "for cityLocalName in cities_ls[:1]:\n",
    "    print(str(cityLocalName))\n",
    "    start_time = datetime.now()\n",
    "    print(str(start_time))\n",
    "\n",
    "    uc_city = uc.query(f'HDENS_NAME == \"{cityLocalName}\"')\n",
    "    ctry_code = uc_city.CNTR_CODE.values.astype(str)[0]\n",
    "    cityLocalName_unicode = unidecode(cityLocalName)\n",
    "    \n",
    "    output_path = os.path.join(outdata_f, f'{ctry_code}_{cityLocalName_unicode}_GQA_centroids.shp')\n",
    "    if not os.path.exists(output_path):\n",
    "        print(f'loading aglomeration city {cityLocalName_unicode}')\n",
    "        agl_city = agls.query(f'agglomerationName_localName == \"{cityLocalName}\"')\n",
    "        # Keep only subset of columns\n",
    "        agl_city = agl_city[['agglomerationId_identifier', 'agglomerationName_nameEng', 'geometry' ]]\n",
    "        if agl_city.empty:\n",
    "            agglomerationId_identifier = 'NotAvailable'\n",
    "            print (\"agglomerationId_identifier\")\n",
    "        else:\n",
    "            agglomerationId_identifier = agl_city.agglomerationId_identifier.values.astype(str)[0]\n",
    "            \n",
    "            print (\"agglomerationId_identifier\")\n",
    "\n",
    "            # Check noise contour maps GeoPackage file\n",
    "            ncm_file_path = os.path.join(indata_f, 'NoiseData', f'Noise_20202025_export_{ctry_code}.gpkg')\n",
    "            #layerName = f'dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_{ctry_code}'\n",
    "            layerName = f'dbodf48_agg_noisecontours_roadsinagglomeration_lden_valid_latestdelivery_poly_{ctry_code}'\n",
    "            ncm = gpd.read_file(ncm_file_path, layer=layerName)\n",
    "            print (\"ncm\")\n",
    "\n",
    "            # subset columns\n",
    "            ncm_gdf = ncm[['category', 'geometry']]\n",
    "\n",
    "            # Perform spatial overlay (intersection) using GeoPandas in parallel with Dask\n",
    "            ncm_agl = parallel_overlay(ncm_gdf, agl_city, npartitions=10, how='intersection')\n",
    "            print(\"ncm_agl\")\n",
    "\n",
    "            # Aggregate the area with lower band values (quieter bands)\n",
    "            ncm_agl_city = parallel_overlay(ncm_agl, agl_city, npartitions=10, how='union')\n",
    "            ncm_agl_city.category.fillna(0)\n",
    "\n",
    "            # Select a subset of columns of interest\n",
    "            ncm_dis = ncm_agl_city[['category', 'geometry']]\n",
    "            \n",
    "            # Define the list of noisy classes\n",
    "            noisy_classes = ['Lden5559', 'Lden6064', 'Lden6569', 'Lden7074', 'LdenGreaterThan75']\n",
    "\n",
    "            # Create a condition based on the category column\n",
    "            condition = ncm_dis['category'].isin(noisy_classes)  # Replace 'category_column' with the actual column name\n",
    "\n",
    "            # Specify the condition and create a new category column based on the condition\n",
    "            ncm_dis['noisy'] = 0\n",
    "            ncm_dis.loc[condition, 'noisy'] = 1\n",
    "            ncm_dis = ncm_dis[['noisy', 'geometry']]\n",
    "            ncm_dis_dg = dg.from_geopandas(ncm_dis, npartitions=10)\n",
    "            ncm_dis = ncm_dis_dg.dissolve(by='noisy').compute().reset_index()\n",
    "            print (\"ncm_dis\")\n",
    "\n",
    "            # 3 READ UA DATA        \n",
    "            # Load GeoPackage info\n",
    "            data_f = r'A:\\Copernicus\\UrbanAtlas\\UrbanAtlas\\UA2018'\n",
    "            ctry_code = uc_city.CNTR_CODE.values.astype(str)[0] \n",
    "            city_unicodeName_upper = unidecode(cityLocalName).upper()\n",
    "            folder_path = glob.glob(os.path.join(data_f, f'{ctry_code}*{city_unicodeName_upper}*'))\n",
    "            ua_file_path =  glob.glob(os.path.join(folder_path[0], 'Data', f'{ctry_code}*{city_unicodeName_upper}*.gpkg'))\n",
    "            layers_ls = fiona.listlayers(ua_file_path[0])\n",
    "            print (\"layers_ls\")\n",
    "\n",
    "            # Read the GeoPackage file\n",
    "            ua = gpd.read_file(ua_file_path[0], layer= layers_ls[0])\n",
    "            print (\"ua\")\n",
    "\n",
    "            # Select 'green' classes\n",
    "            uagreen = ua.query('code_2018 == \"14100\" or code_2018 == \"31000\"')\n",
    "            \n",
    "            # 4 SELECT UA INTERSECTING UC\n",
    "            # Perform spatial overlay (intersection)\n",
    "            uagreen_urbc = parallel_overlay(uagreen, uc_city, npartitions=10, how='intersection')\n",
    "\n",
    "            # 5 IDENTIFY GREEN AREAS EXCLUDED (NOT COVERED BY NCM)\n",
    "            # Perform spatial overlay (intersection)\n",
    "            nqgreen = parallel_overlay(uagreen_urbc, ncm_dis, npartitions=10, how='intersection')\n",
    "            not_covered = uagreen_urbc.geometry.difference(uagreen_urbc.geometry.intersection(nqgreen.geometry.unary_union))\n",
    "            # Filter out empty polygons(not empty polygons)\n",
    "            green_not_covered_by_ncm = not_covered[~not_covered.is_empty]\n",
    "\n",
    "            # save to shapefile\n",
    "            file_path = os.path.join(outdata_f, f'{ctry_code}_{cityLocalName_unicode}_green_not_covered_by_ncm.shp')\n",
    "            green_not_covered_by_ncm.to_file(file_path, driver='ESRI Shapefile')\n",
    "            print (\"green_not_covered_by_ncm\")\n",
    "\n",
    "            # 6 IDENTIFY QUIET/NOISY AREAS\n",
    "            ## for statistics need to calculate area again\n",
    "            # Calculate the area for each geometry and create a new column 'area'\n",
    "            nqgreen['area_m2'] = nqgreen['geometry'].area\n",
    "            nqgreen['area_ha'] = round(nqgreen['area_m2']* 0.0001,2)\n",
    "            nqgreen['area_km2'] = round(nqgreen['area_ha']* 0.01,2)\n",
    "            nqgreen_area = nqgreen.groupby(['code_2018', 'noisy'])['area_m2'].sum().reset_index()\n",
    "            nqgreen_area['area_ha'] = round(nqgreen_area['area_m2']* 0.0001,2)\n",
    "            nqgreen_area['area_km2'] = round(nqgreen_area['area_ha']* 0.01,2)\n",
    "\n",
    "            # 7 EXPORT GREEN QUIET AREAS (GQA)\n",
    "            nqgreen = nqgreen[['country', 'fua_name', 'fua_code', 'HDENS_2011', 'code_2018', 'class_2018', 'noisy',  'area_m2', 'area_ha', 'area_km2', 'geometry']]\n",
    "            GQA = nqgreen.query('noisy == 0')\n",
    "            GNA = nqgreen.query('noisy == 1')\n",
    "\n",
    "            # Export to shapefile\n",
    "            file_path = os.path.join(outdata_f, f'{ctry_code}_{cityLocalName_unicode}_GQA.shp')\n",
    "            GQA.to_file(file_path, driver='ESRI Shapefile')\n",
    "            print (\"GQA\")\n",
    "\n",
    "            # 8 CREATE CENTROIDS FOR GQA POLYGONS\n",
    "            # Create a new GeoDataFrame with centroids as points\n",
    "            GQA_pts = gpd.GeoDataFrame(geometry=GQA['geometry'].centroid)\n",
    "            GQA_pts['oid'] = GQA.index\n",
    "            GQA_pts['fua_name'] = GQA.fua_name\n",
    "            GQA_pts['fua_code'] = GQA.fua_code\n",
    "            GQA_pts['HDENS_2011'] = GQA.HDENS_2011\n",
    "\n",
    "            # Export to shapefile\n",
    "            file_path = os.path.join(outdata_f, f'{ctry_code}_{cityLocalName_unicode}_GQA_centroids.shp')\n",
    "            GQA_pts.to_file(file_path, driver='ESRI Shapefile')\n",
    "\n",
    "            print (\"GQA_pts\")\n",
    "    \n",
    "            # Calculate the duration\n",
    "            end_time = datetime.now()\n",
    "            processing_time = end_time - start_time\n",
    "\n",
    "            print (\"str(processing_time)\")\n",
    "            \n",
    "            ## write output values into log file\n",
    "            uc_km2 = round(uc_city.area.sum()/1000000,2)\n",
    "            agl_city_km2 = round(agl_city.area.sum()/1000000,2)\n",
    "            ncm_agl_city_km2 = round(ncm_agl_city.area.sum()/1000000,2)\n",
    "            ua_km2 = round(ua.area.sum()/1000000,2)\n",
    "            uagreen_km2 = round(uagreen.area.sum()/1000000,2)\n",
    "            uagreen_urbc_km2 = round(uagreen_urbc.area.sum()/1000000,2)\n",
    "            nqgreen_m2 = round(nqgreen.area.sum(),2)\n",
    "            green_not_covered_by_ncm_m2 = round(green_not_covered_by_ncm.area.sum(),2)\n",
    "            GQA_m2 = round(GQA.area.sum(),2)\n",
    "            GNA_m2 = round(GNA.area.sum(),2)\n",
    "            processing_duration = str(processing_time)\n",
    "\n",
    "            log_entry = create_log_entry(cityLocalName, agglomerationId_identifier, uc_km2, agl_city_km2, \n",
    "                                    ncm_agl_city_km2,ua_km2, uagreen_km2, uagreen_urbc_km2, nqgreen_m2, \n",
    "                                    green_not_covered_by_ncm_m2, GQA_m2, GNA_m2, processing_time)\n",
    "            write_log(log_path, log_entry)\n",
    "            # Clean up intermediate variables to free memory\n",
    "            del agl_city, ncm, ncm_agl, ncm_agl_city, ncm_dis, ua, uagreen, uagreen_urbc, nqgreen, green_not_covered_by_ncm, GQA, GNA, GQA_pts\n",
    "\n",
    "            # Shut down the Dask client\n",
    "            client.shutdown()\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ncm_dis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# save to shapefile\u001b[39;00m\n\u001b[0;32m      2\u001b[0m file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(outdata_f, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mctry_code\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mcityLocalName_unicode\u001b[39m}\u001b[39;00m\u001b[39m_ncm_dis.shp\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m ncm_dis\u001b[39m.\u001b[39mto_file(file_path, driver\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mESRI Shapefile\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ncm_dis' is not defined"
     ]
    }
   ],
   "source": [
    "# save to shapefile\n",
    "file_path = os.path.join(outdata_f, f'{ctry_code}_{cityLocalName_unicode}_ncm_dis.shp')\n",
    "ncm_dis.to_file(file_path, driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygeos\n",
      "  Downloading pygeos-0.14-cp39-cp39-win_amd64.whl (1.4 MB)\n",
      "     ---------------------------------------- 1.4/1.4 MB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.13 in \\\\?\\c:\\users\\ubach\\.conda\\envs\\geospatial_etc_di_v4\\lib\\site-packages (from pygeos) (1.24.1)\n",
      "Installing collected packages: pygeos\n",
      "Successfully installed pygeos-0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pygeos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pygeos\n",
    "\n",
    "gpd.options.use_pygeos = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bordeaux\n",
      "2024-07-02 16:51:46.360926\n",
      "loading aglomeration city Bordeaux\n",
      "agglomerationId_identifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 16:51:53,605 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:52:17,175 - distributed.utils_perf - WARNING - full garbage collections took 22% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:52:18,538 - distributed.utils_perf - WARNING - full garbage collections took 23% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:52:22,340 - distributed.utils_perf - WARNING - full garbage collections took 23% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:52:24,539 - distributed.utils_perf - WARNING - full garbage collections took 24% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:52:26,542 - distributed.utils_perf - WARNING - full garbage collections took 24% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:52:28,915 - distributed.utils_perf - WARNING - full garbage collections took 25% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:52:31,340 - distributed.utils_perf - WARNING - full garbage collections took 26% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:52:33,076 - distributed.utils_perf - WARNING - full garbage collections took 26% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:52:37,722 - distributed.utils_perf - WARNING - full garbage collections took 26% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:52:42,968 - distributed.utils_perf - WARNING - full garbage collections took 26% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:52:47,278 - distributed.utils_perf - WARNING - full garbage collections took 26% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:52:50,715 - distributed.utils_perf - WARNING - full garbage collections took 27% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:52:55,486 - distributed.utils_perf - WARNING - full garbage collections took 27% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:53:15,124 - distributed.utils_perf - WARNING - full garbage collections took 24% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:53:18,385 - distributed.utils_perf - WARNING - full garbage collections took 24% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:53:23,526 - distributed.utils_perf - WARNING - full garbage collections took 24% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:53:29,375 - distributed.utils_perf - WARNING - full garbage collections took 23% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:53:35,385 - distributed.utils_perf - WARNING - full garbage collections took 23% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:53:37,125 - distributed.utils_perf - WARNING - full garbage collections took 24% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:53:39,340 - distributed.utils_perf - WARNING - full garbage collections took 24% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:53:45,529 - distributed.utils_perf - WARNING - full garbage collections took 24% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:53:50,955 - distributed.utils_perf - WARNING - full garbage collections took 24% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:53:56,775 - distributed.utils_perf - WARNING - full garbage collections took 23% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:54:03,127 - distributed.utils_perf - WARNING - full garbage collections took 23% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:54:08,478 - distributed.utils_perf - WARNING - full garbage collections took 23% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:54:11,570 - distributed.utils_perf - WARNING - full garbage collections took 23% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:54:13,222 - distributed.utils_perf - WARNING - full garbage collections took 24% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:54:20,756 - distributed.utils_perf - WARNING - full garbage collections took 24% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:54:27,513 - distributed.utils_perf - WARNING - full garbage collections took 24% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:54:29,486 - distributed.utils_perf - WARNING - full garbage collections took 24% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:54:31,226 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:54:34,628 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:54:40,008 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:54:41,711 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:54:45,146 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:54:48,744 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:54:53,516 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:54:58,026 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n",
      "2024-07-02 16:55:03,215 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 16:55:29,065 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncm_agl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q:\\UserTemp\\ubach\\AppData\\Local\\Temp\\63\\ipykernel_52220\\1119770620.py:45: UserWarning: `keep_geom_type=True` in overlay resulted in 9923 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  overlays = [gpd.overlay(gdf1_part, gdf2_part, how=how) for gdf1_part in gdf1_parts for gdf2_part in gdf2_parts]\n",
      "Q:\\UserTemp\\ubach\\AppData\\Local\\Temp\\63\\ipykernel_52220\\1119770620.py:45: UserWarning: `keep_geom_type=True` in overlay resulted in 3905 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  overlays = [gpd.overlay(gdf1_part, gdf2_part, how=how) for gdf1_part in gdf1_parts for gdf2_part in gdf2_parts]\n",
      "Q:\\UserTemp\\ubach\\AppData\\Local\\Temp\\63\\ipykernel_52220\\1119770620.py:45: UserWarning: `keep_geom_type=True` in overlay resulted in 2721 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  overlays = [gpd.overlay(gdf1_part, gdf2_part, how=how) for gdf1_part in gdf1_parts for gdf2_part in gdf2_parts]\n",
      "c:\\Users\\ubach\\.conda\\envs\\geospatial_ETC_DI_v4\\lib\\site-packages\\geopandas\\geodataframe.py:1443: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncm_dis\n",
      "layers_ls\n",
      "ua\n",
      "green_not_covered_by_ncm\n",
      "GQA\n",
      "GQA_pts\n",
      "str(processing_time)\n"
     ]
    }
   ],
   "source": [
    "cities_ls = ['Kaunas', 'København', 'Girona', 'Bordeaux']\n",
    "# Loop through test cities\n",
    "for cityLocalName in cities_ls[3:4]:\n",
    "    print(str(cityLocalName))\n",
    "    start_time = datetime.now()\n",
    "    print(str(start_time))\n",
    "    uc_city = uc.query(f'HDENS_NAME == \"{cityLocalName}\"')\n",
    "    ctry_code = uc_city.CNTR_CODE.values.astype(str)[0]\n",
    "    cityLocalName_unicode = unidecode(cityLocalName)\n",
    "    output_path = os.path.join(outdata_f, f'{ctry_code}_{cityLocalName_unicode}_GQA_centroids.shp')\n",
    "    if not os.path.exists(output_path):\n",
    "        print(f'loading aglomeration city {cityLocalName_unicode}')\n",
    "        agl_city = agls.query(f'agglomerationName_localName == \"{cityLocalName}\"')\n",
    "        # Keep only subset of columns\n",
    "        agl_city = agl_city[['agglomerationId_identifier', 'agglomerationName_nameEng', 'geometry' ]]\n",
    "        if agl_city.empty:\n",
    "            agglomerationId_identifier = 'NotAvailable'\n",
    "            print (\"agglomerationId_identifier\")\n",
    "        else:\n",
    "            agglomerationId_identifier = agl_city.agglomerationId_identifier.values.astype(str)[0]\n",
    "            \n",
    "            print (\"agglomerationId_identifier\")\n",
    "\n",
    "            # Check noise contour maps GeoPackage file\n",
    "            ncm_file_path = os.path.join(indata_f, 'NoiseData', f'Noise_20202025_export.gpkg')\n",
    "            #ncm_file_path = os.path.join(indata_f, 'NoiseData', f'Noise_20202025_export_{ctry_code}.gpkg')\n",
    "            layerName = f'dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_{ctry_code}'\n",
    "            #layerName = f'dbodf48_agg_noisecontours_roadsinagglomeration_lden_valid_latestdelivery_poly_{ctry_code}'\n",
    "            ncm = gpd.read_file(ncm_file_path, layer=layerName)\n",
    "            print (\"ncm\")\n",
    "\n",
    "            # subset columns\n",
    "            ncm_gdf = ncm[['category', 'geometry']]\n",
    "\n",
    "            # Perform spatial overlay (intersection) using GeoPandas in parallel with Dask\n",
    "            ncm_agl = parallel_overlay(ncm_gdf, agl_city, npartitions=10, how='intersection')\n",
    "            print(\"ncm_agl\")\n",
    "\n",
    "            # Aggregate the area with lower band values (quieter bands)\n",
    "            ncm_agl_city = parallel_overlay(ncm_agl, agl_city, npartitions=10, how='union')\n",
    "            ncm_agl_city.category.fillna(0)\n",
    "\n",
    "            # Select a subset of columns of interest\n",
    "            ncm_dis = ncm_agl_city[['category', 'geometry']]\n",
    "            \n",
    "            # Define the list of noisy classes\n",
    "            noisy_classes = ['Lden5559', 'Lden6064', 'Lden6569', 'Lden7074', 'LdenGreaterThan75']\n",
    "\n",
    "            # Create a condition based on the category column\n",
    "            condition = ncm_dis['category'].isin(noisy_classes)  # Replace 'category_column' with the actual column name\n",
    "\n",
    "            # Specify the condition and create a new category column based on the condition\n",
    "            ncm_dis['noisy'] = 0\n",
    "            ncm_dis.loc[condition, 'noisy'] = 1\n",
    "            ncm_dis = ncm_dis[['noisy', 'geometry']]\n",
    "            ncm_dis_dg = dg.from_geopandas(ncm_dis, npartitions=10)\n",
    "            ncm_dis = ncm_dis_dg.dissolve(by='noisy').compute().reset_index()\n",
    "            print (\"ncm_dis\")\n",
    "\n",
    "            # 3 READ UA DATA        \n",
    "            # Load GeoPackage info\n",
    "            data_f = r'A:\\Copernicus\\UrbanAtlas\\UrbanAtlas\\UA2018'\n",
    "            ctry_code = uc_city.CNTR_CODE.values.astype(str)[0] \n",
    "            city_unicodeName_upper = unidecode(cityLocalName).upper()\n",
    "            folder_path = glob.glob(os.path.join(data_f, f'{ctry_code}*{city_unicodeName_upper}*'))\n",
    "            ua_file_path =  glob.glob(os.path.join(folder_path[0], 'Data', f'{ctry_code}*{city_unicodeName_upper}*.gpkg'))\n",
    "            layers_ls = fiona.listlayers(ua_file_path[0])\n",
    "            print (\"layers_ls\")\n",
    "\n",
    "            # Read the GeoPackage file\n",
    "            ua = gpd.read_file(ua_file_path[0], layer= layers_ls[0])\n",
    "            print (\"ua\")\n",
    "\n",
    "            # Select 'green' classes\n",
    "            uagreen = ua.query('code_2018 == \"14100\" or code_2018 == \"31000\"')\n",
    "            \n",
    "            # 4 SELECT UA INTERSECTING UC\n",
    "            # Perform spatial overlay (intersection)\n",
    "            uagreen_urbc = parallel_overlay(uagreen, uc_city, npartitions=10, how='intersection')\n",
    "\n",
    "            # 5 IDENTIFY GREEN AREAS EXCLUDED (NOT COVERED BY NCM)\n",
    "            # Perform spatial overlay (intersection)\n",
    "            nqgreen = parallel_overlay(uagreen_urbc, ncm_dis, npartitions=10, how='intersection')\n",
    "            not_covered = uagreen_urbc.geometry.difference(uagreen_urbc.geometry.intersection(nqgreen.geometry.unary_union))\n",
    "            # Filter out empty polygons(not empty polygons)\n",
    "            green_not_covered_by_ncm = not_covered[~not_covered.is_empty]\n",
    "\n",
    "            # save to shapefile\n",
    "            file_path = os.path.join(outdata_f, f'{ctry_code}_{cityLocalName_unicode}_green_not_covered_by_ncm.shp')\n",
    "            green_not_covered_by_ncm.to_file(file_path, driver='ESRI Shapefile')\n",
    "            print (\"green_not_covered_by_ncm\")\n",
    "\n",
    "            # 6 IDENTIFY QUIET/NOISY AREAS\n",
    "            ## for statistics need to calculate area again\n",
    "            # Calculate the area for each geometry and create a new column 'area'\n",
    "            nqgreen['area_m2'] = nqgreen['geometry'].area\n",
    "            nqgreen['area_ha'] = round(nqgreen['area_m2']* 0.0001,2)\n",
    "            nqgreen['area_km2'] = round(nqgreen['area_ha']* 0.01,2)\n",
    "            nqgreen_area = nqgreen.groupby(['code_2018', 'noisy'])['area_m2'].sum().reset_index()\n",
    "            nqgreen_area['area_ha'] = round(nqgreen_area['area_m2']* 0.0001,2)\n",
    "            nqgreen_area['area_km2'] = round(nqgreen_area['area_ha']* 0.01,2)\n",
    "\n",
    "            # 7 EXPORT GREEN QUIET AREAS (GQA)\n",
    "            nqgreen = nqgreen[['country', 'fua_name', 'fua_code', 'HDENS_2011', 'code_2018', 'class_2018', 'noisy',  'area_m2', 'area_ha', 'area_km2', 'geometry']]\n",
    "            GQA = nqgreen.query('noisy == 0')\n",
    "            GNA = nqgreen.query('noisy == 1')\n",
    "\n",
    "            # Export to shapefile\n",
    "            file_path = os.path.join(outdata_f, f'{ctry_code}_{cityLocalName_unicode}_GQA.shp')\n",
    "            GQA.to_file(file_path, driver='ESRI Shapefile')\n",
    "            print (\"GQA\")\n",
    "\n",
    "            # 8 CREATE CENTROIDS FOR GQA POLYGONS\n",
    "            # Create a new GeoDataFrame with centroids as points\n",
    "            GQA_pts = gpd.GeoDataFrame(geometry=GQA['geometry'].centroid)\n",
    "            GQA_pts['oid'] = GQA.index\n",
    "            GQA_pts['fua_name'] = GQA.fua_name\n",
    "            GQA_pts['fua_code'] = GQA.fua_code\n",
    "            GQA_pts['HDENS_2011'] = GQA.HDENS_2011\n",
    "\n",
    "            # Export to shapefile\n",
    "            file_path = os.path.join(outdata_f, f'{ctry_code}_{cityLocalName_unicode}_GQA_centroids.shp')\n",
    "            GQA_pts.to_file(file_path, driver='ESRI Shapefile')\n",
    "\n",
    "            print (\"GQA_pts\")\n",
    "    \n",
    "            # Calculate the duration\n",
    "            end_time = datetime.now()\n",
    "            processing_time = end_time - start_time\n",
    "\n",
    "            print (\"str(processing_time)\")\n",
    "            \n",
    "            ## write output values into log file\n",
    "            uc_km2 = round(uc_city.area.sum()/1000000,2)\n",
    "            agl_city_km2 = round(agl_city.area.sum()/1000000,2)\n",
    "            ncm_agl_city_km2 = round(ncm_agl_city.area.sum()/1000000,2)\n",
    "            ua_km2 = round(ua.area.sum()/1000000,2)\n",
    "            uagreen_km2 = round(uagreen.area.sum()/1000000,2)\n",
    "            uagreen_urbc_km2 = round(uagreen_urbc.area.sum()/1000000,2)\n",
    "            nqgreen_m2 = round(nqgreen.area.sum(),2)\n",
    "            green_not_covered_by_ncm_m2 = round(green_not_covered_by_ncm.area.sum(),2)\n",
    "            GQA_m2 = round(GQA.area.sum(),2)\n",
    "            GNA_m2 = round(GNA.area.sum(),2)\n",
    "            processing_duration = str(processing_time)\n",
    "\n",
    "            log_entry = create_log_entry(cityLocalName, agglomerationId_identifier, uc_km2, agl_city_km2, \n",
    "                                    ncm_agl_city_km2,ua_km2, uagreen_km2, uagreen_urbc_km2, nqgreen_m2, \n",
    "                                    green_not_covered_by_ncm_m2, GQA_m2, GNA_m2, processing_time)\n",
    "            write_log(log_path, log_entry)\n",
    "            # Clean up intermediate variables to free memory\n",
    "            del agl_city, ncm, ncm_agl, ncm_agl_city, ncm_dis, ua, uagreen, uagreen_urbc, nqgreen, green_not_covered_by_ncm, GQA, GNA, GQA_pts\n",
    "\n",
    "            # Shut down the Dask client\n",
    "            client.shutdown()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_ES\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_FR\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_CY\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_BE\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_CZ\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_DK\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_EE\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_FI\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_IE\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_LT\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_LV\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_SE\n",
      "dbo.DF15_AgglomerationSource_Valid_LatestDelivery_polygon\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_PL\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_DE\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_CH\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_HR\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_LU\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_NO\n",
      "dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_PT\n"
     ]
    }
   ],
   "source": [
    "import fiona\n",
    "\n",
    "gpkg_path =  os.path.join(indata_f, 'NoiseData', f'Noise_20202025_export.gpkg')\n",
    "layers = fiona.listlayers(gpkg_path)\n",
    "\n",
    "for layer_name in layers:\n",
    "    print(layer_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Null layer: 'dbodf48_agg_noisecontours_roadsinagglomeration_lden_valid_latestdelivery_poly_FR'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39m#layerName = f'dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_{ctry_code}'\u001b[39;00m\n\u001b[0;32m      5\u001b[0m layerName \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdbodf48_agg_noisecontours_roadsinagglomeration_lden_valid_latestdelivery_poly_\u001b[39m\u001b[39m{\u001b[39;00mctry_code\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m ncm \u001b[39m=\u001b[39m gpd\u001b[39m.\u001b[39;49mread_file(ncm_file_path, layer\u001b[39m=\u001b[39;49mlayerName)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mncm\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ubach\\.conda\\envs\\geospatial_ETC_DI_v4\\lib\\site-packages\\geopandas\\io\\file.py:259\u001b[0m, in \u001b[0;36m_read_file\u001b[1;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m     path_or_bytes \u001b[39m=\u001b[39m filename\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m engine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfiona\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 259\u001b[0m     \u001b[39mreturn\u001b[39;00m _read_file_fiona(\n\u001b[0;32m    260\u001b[0m         path_or_bytes, from_bytes, bbox\u001b[39m=\u001b[39mbbox, mask\u001b[39m=\u001b[39mmask, rows\u001b[39m=\u001b[39mrows, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    261\u001b[0m     )\n\u001b[0;32m    262\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyogrio\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    263\u001b[0m     \u001b[39mreturn\u001b[39;00m _read_file_pyogrio(\n\u001b[0;32m    264\u001b[0m         path_or_bytes, bbox\u001b[39m=\u001b[39mbbox, mask\u001b[39m=\u001b[39mmask, rows\u001b[39m=\u001b[39mrows, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    265\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ubach\\.conda\\envs\\geospatial_ETC_DI_v4\\lib\\site-packages\\geopandas\\io\\file.py:303\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[1;34m(path_or_bytes, from_bytes, bbox, mask, rows, where, **kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     reader \u001b[39m=\u001b[39m fiona\u001b[39m.\u001b[39mopen\n\u001b[0;32m    302\u001b[0m \u001b[39mwith\u001b[39;00m fiona_env():\n\u001b[1;32m--> 303\u001b[0m     \u001b[39mwith\u001b[39;00m reader(path_or_bytes, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mas\u001b[39;00m features:\n\u001b[0;32m    304\u001b[0m         crs \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mcrs_wkt\n\u001b[0;32m    305\u001b[0m         \u001b[39m# attempt to get EPSG code\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ubach\\.conda\\envs\\geospatial_ETC_DI_v4\\lib\\site-packages\\fiona\\env.py:408\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    406\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    407\u001b[0m     \u001b[39mif\u001b[39;00m local\u001b[39m.\u001b[39m_env:\n\u001b[1;32m--> 408\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    409\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ubach\\.conda\\envs\\geospatial_ETC_DI_v4\\lib\\site-packages\\fiona\\__init__.py:264\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     path \u001b[39m=\u001b[39m parse_path(fp)\n\u001b[0;32m    263\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 264\u001b[0m     c \u001b[39m=\u001b[39m Collection(path, mode, driver\u001b[39m=\u001b[39mdriver, encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m    265\u001b[0m                    layer\u001b[39m=\u001b[39mlayer, enabled_drivers\u001b[39m=\u001b[39menabled_drivers, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    266\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    267\u001b[0m     \u001b[39mif\u001b[39;00m schema:\n\u001b[0;32m    268\u001b[0m         \u001b[39m# Make an ordered dict of schema properties.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ubach\\.conda\\envs\\geospatial_ETC_DI_v4\\lib\\site-packages\\fiona\\collection.py:162\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[1;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    161\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m Session()\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mstart(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    163\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    164\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m WritingSession()\n",
      "File \u001b[1;32mfiona\\ogrext.pyx:553\u001b[0m, in \u001b[0;36mfiona.ogrext.Session.start\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Null layer: 'dbodf48_agg_noisecontours_roadsinagglomeration_lden_valid_latestdelivery_poly_FR'"
     ]
    }
   ],
   "source": [
    "# Check noise contour maps GeoPackage file\n",
    "#ncm_file_path = os.path.join(indata_f, 'NoiseData', f'Noise_20202025_export_{ctry_code}.gpkg')\n",
    "ncm_file_path = os.path.join(indata_f, 'NoiseData', f'Noise_20202025_export.gpkg')\n",
    "#layerName = f'dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_{ctry_code}'\n",
    "layerName = f'dbodf48_agg_noisecontours_roadsinagglomeration_lden_valid_latestdelivery_poly_{ctry_code}'\n",
    "ncm = gpd.read_file(ncm_file_path, layer=layerName)\n",
    "print (\"ncm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_ETC_DI_v4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
