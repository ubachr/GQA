{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data folders\n",
    "indata_f = r'P:\\Environment and Health\\Noise\\ServiceContract\\2024_ServiceContract\\QuietAreas'\n",
    "outdata_f = os.path.join(indata_f, 'OutputData', 'test0207')\n",
    "if not os.path.exists(outdata_f):\n",
    "    # Create the folder if it doesn't exist\n",
    "    os.makedirs(outdata_f)\n",
    "\n",
    "# 0 PREPARE A LOG FILE FOR QC\n",
    "log_file = 'log_GQA_Step1.csv'\n",
    "log_path = os.path.join(outdata_f, log_file)\n",
    "\n",
    "# Initialize Dask client\n",
    "client = Client()\n",
    "\n",
    "# 1 READ URBAN CENTRES\n",
    "# Read shapefile\n",
    "uc_file_path = os.path.join(indata_f, 'UrbanCentres', 'HDC2021_RG.shp')\n",
    "# Read the GeoPackage file\n",
    "uc = gpd.read_file(uc_file_path)\n",
    "\n",
    "\n",
    "# 2 READ NOISE DATA\n",
    "# Load agglomerations delineations\n",
    "agls_file_path = os.path.join(indata_f, 'NoiseData', 'DF1_5_Agglomerations_20240429.gpkg')\n",
    "\n",
    "# Read the GeoPackage file\n",
    "agls = gpd.read_file(agls_file_path, layer = 'dbo.DF15_AgglomerationSource_Valid_LatestDelivery')\n",
    "\n",
    "cities_ls = ['Kaunas', 'KÃ¸benhavn', 'Girona', 'Bordeaux']\n",
    "# Loop through test cities\n",
    "for cityLocalName in cities_ls[:1]:\n",
    "    print(str(cityLocalName))\n",
    "    start_time = datetime.now()\n",
    "    print(str(start_time))\n",
    "    uc_city = uc.query(f'HDENS_NAME == \"{cityLocalName}\"')\n",
    "    ctry_code = uc_city.CNTR_CODE.values.astype(str)[0]\n",
    "    cityLocalName_unicode = unidecode(cityLocalName)\n",
    "    output_path = os.path.join(outdata_f, f'{ctry_code}_{cityLocalName_unicode}_GQA_centroids.shp')\n",
    "    if not os.path.exists(output_path):\n",
    "        print(f'loading aglomeration city {cityLocalName_unicode}')\n",
    "        agl_city = agls.query(f'agglomerationName_localName == \"{cityLocalName}\"')\n",
    "        # Keep only subset of columns\n",
    "        agl_city = agl_city[['agglomerationId_identifier', 'agglomerationName_nameEng', 'geometry' ]]\n",
    "        if agl_city.empty:\n",
    "            agglomerationId_identifier = 'NotAvailable'\n",
    "            print (\"agglomerationId_identifier\")\n",
    "        else:\n",
    "            agglomerationId_identifier = agl_city.agglomerationId_identifier.values.astype(str)[0]\n",
    "            \n",
    "            print (\"agglomerationId_identifier\")\n",
    "\n",
    "            # Check noise contour maps GeoPackage file\n",
    "            ncm_file_path = os.path.join(indata_f, 'NoiseData', f'Noise_20202025_export_{ctry_code}.gpkg')\n",
    "            #layerName = f'dbo.DF48_agg_NoiseContours_roadsInAgglomeration_Lden_Valid_LatestDelivery_Poly_{ctry_code}'\n",
    "            layerName = f'dbodf48_agg_noisecontours_roadsinagglomeration_lden_valid_latestdelivery_poly_{ctry_code}'\n",
    "            ncm = gpd.read_file(ncm_file_path, layer=layerName)\n",
    "            print (\"ncm\")\n",
    "\n",
    "            # subset columns\n",
    "            ncm_gdf = ncm[['category', 'geometry']]\n",
    "\n",
    "            # Convert GeoDataFrames to Dask GeoDataFrames\n",
    "            ncm = dg.from_geopandas(ncm_gdf, npartitions=10)\n",
    "            agl_city = dg.from_geopandas(agl_city, npartitions=10)\n",
    "    \n",
    "            # Perform spatial overlay (intersection) \n",
    "            # Perform overlay operation\n",
    "            dask_overlay = dg.overlay(ncm, agl_city, how='intersection')\n",
    "            ncm_agl = dask_overlay.compute()\n",
    "            print (\"ncm_agl\")\n",
    "\n",
    "            # Aggregate the area with lower band values (quieter bands)\n",
    "            ncm_agl_city = dg.overlay(ncm_agl, agl_city, how='union')\n",
    "            ncm_agl_city = ncm_agl_city.compute()\n",
    "            ncm_agl_city.category.fillna(0)\n",
    "\n",
    "            # Select a subset of columns of interest\n",
    "            ncm_dis = ncm_agl_city[['category', 'geometry']]\n",
    "            \n",
    "            # Define the list of noisy classes\n",
    "            noisy_classes = ['Lden5559', 'Lden6064', 'Lden6569', 'Lden7074', 'LdenGreaterThan75']\n",
    "\n",
    "            # Create a condition based on the category column\n",
    "            condition = ncm_dis['category'].isin(noisy_classes)  # Replace 'category_column' with the actual column name\n",
    "\n",
    "            # Specify the condition and create a new category column based on the condition\n",
    "            ncm_dis['noisy'] = 0\n",
    "            ncm_dis.loc[condition, 'noisy'] = 1\n",
    "            ncm_dis = ncm_dis[['noisy', 'geometry']]\n",
    "            ncm_dis_dg = dg.from_geopandas(ncm_dis, npartitions=10)\n",
    "            ncm_dis = ncm_dis_dg.dissolve(by='noisy').compute().reset_index()\n",
    "            print (\"ncm_dis\")\n",
    "\n",
    "            # 3 READ UA DATA        \n",
    "            # Load GeoPackage info\n",
    "            data_f = r'A:\\Copernicus\\UrbanAtlas\\UrbanAtlas\\UA2018'\n",
    "            ctry_code = uc_city.CNTR_CODE.values.astype(str)[0] \n",
    "            city_unicodeName_upper = unidecode(cityLocalName).upper()\n",
    "            folder_path = glob.glob(os.path.join(data_f, f'{ctry_code}*{city_unicodeName_upper}*'))\n",
    "            ua_file_path =  glob.glob(os.path.join(folder_path[0], 'Data', f'{ctry_code}*{city_unicodeName_upper}*.gpkg'))\n",
    "            layers_ls = fiona.listlayers(ua_file_path[0])\n",
    "            print (\"layers_ls\")\n",
    "\n",
    "            # Read the GeoPackage file\n",
    "            ua = gpd.read_file(ua_file_path[0], layer= layers_ls[0])\n",
    "            print (\"ua\")\n",
    "\n",
    "            # Select 'green' classes\n",
    "            uagreen = ua.query('code_2018 == \"14100\" or code_2018 == \"31000\"')\n",
    "            uagreen_dg = dg.from_geopandas(uagreen, npartitions=10)\n",
    "            \n",
    "            # 4 SELECT UA INTERSECTING UC\n",
    "            # Perform spatial overlay (intersection)\n",
    "            uc_city_dg = dg.from_geopandas(uc_city, npartitions=10)\n",
    "            uagreen_urbc = dg.overlay(uagreen_dg, uc_city_dg, how='intersection').compute()\n",
    "\n",
    "            # 5 IDENTIFY GREEN AREAS EXCLUDED (NOT COVERED BY NCM)\n",
    "            # Perform spatial overlay (intersection)\n",
    "            nqgreen = dg.overlay(uagreen_urbc, ncm_dis, how='intersection').compute() #noisy/quiet green\n",
    "            not_covered = uagreen_urbc.geometry.difference(uagreen_urbc.geometry.intersection(nqgreen.geometry.unary_union))\n",
    "            # Filter out empty polygons(not empty polygons)\n",
    "            green_not_covered_by_ncm = not_covered[~not_covered.is_empty]\n",
    "\n",
    "            # save to shapefile\n",
    "            file_path = os.path.join(outdata_f, f'{ctry_code}_{cityLocalName_unicode}_green_not_covered_by_ncm.shp')\n",
    "            green_not_covered_by_ncm.to_file(file_path, driver='ESRI Shapefile')\n",
    "            print (\"green_not_covered_by_ncm\")\n",
    "\n",
    "            # 6 IDENTIFY QUIET/NOISY AREAS\n",
    "            ## for statistics need to calculate area again\n",
    "            # Calculate the area for each geometry and create a new column 'area'\n",
    "            nqgreen['area_m2'] = nqgreen['geometry'].area\n",
    "            nqgreen['area_ha'] = round(nqgreen['area_m2']* 0.0001,2)\n",
    "            nqgreen['area_km2'] = round(nqgreen['area_ha']* 0.01,2)\n",
    "            nqgreen_area = nqgreen.groupby(['code_2018', 'noisy'])['area_m2'].sum().reset_index()\n",
    "            nqgreen_area['area_ha'] = round(nqgreen_area['area_m2']* 0.0001,2)\n",
    "            nqgreen_area['area_km2'] = round(nqgreen_area['area_ha']* 0.01,2)\n",
    "\n",
    "            # 7 EXPORT GREEN QUIET AREAS (GQA)\n",
    "            nqgreen = nqgreen[['country', 'fua_name', 'fua_code', 'HDENS_2011', 'code_2018', 'class_2018', 'noisy',  'area_m2', 'area_ha', 'area_km2', 'geometry']]\n",
    "            GQA = nqgreen.query('noisy == 0')\n",
    "            GNA = nqgreen.query('noisy == 1')\n",
    "\n",
    "            # Export to shapefile\n",
    "            file_path = os.path.join(outdata_f, f'{ctry_code}_{cityLocalName_unicode}_GQA.shp')\n",
    "            GQA.to_file(file_path, driver='ESRI Shapefile')\n",
    "            print (\"GQA\")\n",
    "\n",
    "            # 8 CREATE CENTROIDS FOR GQA POLYGONS\n",
    "            # Create a new GeoDataFrame with centroids as points\n",
    "            GQA_pts = gpd.GeoDataFrame(geometry=GQA['geometry'].centroid)\n",
    "            GQA_pts['oid'] = GQA.index\n",
    "            GQA_pts['fua_name'] = GQA.fua_name\n",
    "            GQA_pts['fua_code'] = GQA.fua_code\n",
    "            GQA_pts['HDENS_2011'] = GQA.HDENS_2011\n",
    "\n",
    "            # Export to shapefile\n",
    "            file_path = os.path.join(outdata_f, f'{ctry_code}_{cityLocalName_unicode}_GQA_centroids.shp')\n",
    "            GQA_pts.to_file(file_path, driver='ESRI Shapefile')\n",
    "\n",
    "            print (\"GQA_pts\")\n",
    "    \n",
    "            # Calculate the duration\n",
    "            end_time = datetime.now()\n",
    "            processing_time = end_time - start_time\n",
    "\n",
    "            print (\"str(processing_time)\")\n",
    "            \n",
    "            ## write output values into log file\n",
    "            uc_km2 = round(uc_city.area.sum()/1000000,2)\n",
    "            agl_city_km2 = round(agl_city.area.sum()/1000000,2)\n",
    "            ncm_agl_city_km2 = round(ncm_agl_city.area.sum()/1000000,2)\n",
    "            ua_km2 = round(ua.area.sum()/1000000,2)\n",
    "            uagreen_km2 = round(uagreen.area.sum()/1000000,2)\n",
    "            uagreen_urbc_km2 = round(uagreen_urbc.area.sum()/1000000,2)\n",
    "            nqgreen_m2 = round(nqgreen.area.sum(),2)\n",
    "            green_not_covered_by_ncm_m2 = round(green_not_covered_by_ncm.area.sum(),2)\n",
    "            GQA_m2 = round(GQA.area.sum(),2)\n",
    "            GNA_m2 = round(GNA.area.sum(),2)\n",
    "            processing_duration = str(processing_time)\n",
    "\n",
    "            log_entry = create_log_entry(cityLocalName, agglomerationId_identifier, uc_km2, agl_city_km2, \n",
    "                                    ncm_agl_city_km2,ua_km2, uagreen_km2, uagreen_urbc_km2, nqgreen_m2, \n",
    "                                    green_not_covered_by_ncm_m2, GQA_m2, GNA_m2, processing_time)\n",
    "            write_log(log_path, log_entry)\n",
    "            # Clean up intermediate variables to free memory\n",
    "            del agl_city, ncm, ncm_agl, ncm_agl_city, ncm_dis, ua, uagreen, uagreen_urbc, nqgreen, green_not_covered_by_ncm, GQA, GNA, GQA_pts\n",
    "\n",
    "            # Shut down the Dask client\n",
    "            client.shutdown()\n",
    "\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_ETC_DI_v4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
